{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import TradingEnv, StocksEnv, Actions, Positions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price   volume    sma\n",
      "0  37.35  3138230  44.90\n",
      "1  37.31  2044994  44.95\n",
      "2  36.22  2235402  45.01\n",
      "3  37.35  2123003  45.06\n",
      "4  35.77  2015776  45.12\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "t = \"A\"\n",
    "df = pd.read_csv(f\"data/{t}.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup env\n",
    "def mprocess_data(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, \"price\"].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:, [\"price\", \"volume\", \"sma\"]].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "\n",
    "class MStocksEnv(StocksEnv):\n",
    "    _process_data = mprocess_data\n",
    "\n",
    "\n",
    "env = MStocksEnv(df=df, window_size=200, frame_bound=(200, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env information:\n",
      "> shape: (200, 3)\n",
      "> df.shape: (6097, 3)\n",
      "> prices.shape: (6097,)\n",
      "> signal_features.shape: (6097, 3)\n",
      "> max_possible_profit: 1.928644751063614e+19\n"
     ]
    }
   ],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env.unwrapped.shape)\n",
    "print(\"> df.shape:\", env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env.unwrapped.max_possible_profit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPISODES = 10\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_UPDATE_FREQ = 10  # episodes\n",
    "MEMORY_CAPACITY = 10000\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        # A simple MLP; you can customize this architecture\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess environment observation\n",
    "def preprocess_obs(obs):\n",
    "    # Assume obs is a 2D array with shape (window_size, num_features)\n",
    "    # Flatten the observation to a 1D vector\n",
    "    return np.array(obs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "def train():\n",
    "    obs_space_shape = env.observation_space.shape  # e.g. (window_size, num_features)\n",
    "    input_dim = np.prod(obs_space_shape)  # flatten the observation\n",
    "    output_dim = env.action_space.n         # 2 actions: Sell (0) and Buy (1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    policy_net = DQN(input_dim, output_dim).to(device)\n",
    "    target_net = DQN(input_dim, output_dim).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "    epsilon = EPS_START\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        obs, info = env.reset(seed=episode)\n",
    "        obs = preprocess_obs(obs)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "                    q_values = policy_net(state_tensor)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_obs_flat = preprocess_obs(next_obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Save transition\n",
    "            memory.push((obs, action, reward, next_obs_flat, done))\n",
    "            obs = next_obs_flat\n",
    "\n",
    "            # Perform a training step\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                transitions = memory.sample(BATCH_SIZE)\n",
    "                batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = zip(*transitions)\n",
    "\n",
    "                batch_obs = torch.FloatTensor(batch_obs).to(device)\n",
    "                batch_actions = torch.LongTensor(batch_actions).unsqueeze(1).to(device)\n",
    "                batch_rewards = torch.FloatTensor(batch_rewards).unsqueeze(1).to(device)\n",
    "                batch_next_obs = torch.FloatTensor(batch_next_obs).to(device)\n",
    "                batch_dones = torch.FloatTensor(batch_dones).unsqueeze(1).to(device)\n",
    "\n",
    "                # Compute current Q values\n",
    "                current_q = policy_net(batch_obs).gather(1, batch_actions)\n",
    "\n",
    "                # Compute target Q values using the target network\n",
    "                with torch.no_grad():\n",
    "                    max_next_q = target_net(batch_next_obs).max(1, keepdim=True)[0]\n",
    "                    target_q = batch_rewards + GAMMA * max_next_q * (1 - batch_dones)\n",
    "\n",
    "                # Compute loss (MSE)\n",
    "                loss = nn.MSELoss()(current_q, target_q)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "        print(f\"Episode {episode+1}/{NUM_EPISODES}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "        # Update the target network periodically\n",
    "        if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Save the trained model if desired\n",
    "    torch.save(policy_net.state_dict(), \"dqn_trading_model.pth\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10, Total Reward: -1.22, Epsilon: 0.995\n",
      "Episode 2/10, Total Reward: 1.21, Epsilon: 0.990\n",
      "Episode 3/10, Total Reward: 32.52, Epsilon: 0.985\n",
      "Episode 4/10, Total Reward: 30.76, Epsilon: 0.980\n",
      "Episode 5/10, Total Reward: 152.89, Epsilon: 0.975\n",
      "Episode 6/10, Total Reward: 12.34, Epsilon: 0.970\n",
      "Episode 7/10, Total Reward: 135.65, Epsilon: 0.966\n",
      "Episode 8/10, Total Reward: 47.42, Epsilon: 0.961\n",
      "Episode 9/10, Total Reward: 64.32, Epsilon: 0.956\n",
      "Episode 10/10, Total Reward: 94.06, Epsilon: 0.951\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
